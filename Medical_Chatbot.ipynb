{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOa+J5pRqe0uxFsvDrhP6/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnan417/Medical_Chatbot/blob/main/Medical_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "bz2sasqzkLkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le5H0iRYcIMf"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu transformers pypdf gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pdf and convert it into chunks"
      ],
      "metadata": {
        "id": "MQgyld0hkVq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"Medical_book.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "npuON3WkcZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert chunks into vector db and load into a variable vector_db"
      ],
      "metadata": {
        "id": "unTIB2RvkcCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = FAISS.from_documents(chunks, embedding)\n"
      ],
      "metadata": {
        "id": "311nJivndNxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save embeddings to disk"
      ],
      "metadata": {
        "id": "wJ7CKxztkjAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.save_local('faiss_index')"
      ],
      "metadata": {
        "id": "avPCWVcadYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup llm"
      ],
      "metadata": {
        "id": "KYs5R9CPkmRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256)\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n"
      ],
      "metadata": {
        "id": "Q10alh4Ld5te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create prompt template"
      ],
      "metadata": {
        "id": "GNjl4T-Ckrku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a knowledgeable medical assistant.\n",
        "Use only the information from the provided context to answer the user's question.\n",
        "If the answer is not found in the context, say \"I dont know. The question is out of context\"\n",
        "Do NOT use any external knowledge or make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cPTOOqnzgaL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup retrieval chain"
      ],
      "metadata": {
        "id": "sUCAWcTqkuHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ],
      "metadata": {
        "id": "53UpD4Y8eOy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query"
      ],
      "metadata": {
        "id": "AA6m7n7ikxb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the cure for diabetes?\"\n",
        "response = qa_chain.invoke(query)\n",
        "print(response['result'])\n"
      ],
      "metadata": {
        "id": "gQt4JnAiebiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acXGW_J4e9km"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}